{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **OJKGPT**\n",
    "\n",
    "This is the Jupyter Notebook for the development of the 1st version of OJKGPT for the showcase demo capabilites to wealth team program. <br>\n",
    "In this development, we use several data sources for enriching the RAG system of the chatbot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.models_definer import ModelName\n",
    "\n",
    "# storing or not\n",
    "OCR_TRESHOLD = 0.98\n",
    "TOP_K = 3\n",
    "NUM_BATCH_EVAL = 10\n",
    "STORE = True\n",
    "DELETE = True\n",
    "EVAL = False\n",
    "model_name = ModelName.AZURE_OPENAI\n",
    "model_name_eval = ModelName.OPENAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LLM Models Define**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from utils.models_definer import get_llm_and_embedding\n",
    "\n",
    "# ollama/openai/azure_openai\n",
    "llm,embedding_llm = get_llm_and_embedding(model_name=model_name)\n",
    "\n",
    "llm_eval = get_llm_and_embedding(model_name=model_name_eval)[0]\n",
    "\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embedding_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Loading Documents**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Document Reader and Node Parser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuPDF error: format error: cmsOpenProfileFromMem failed\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m STORE:\n\u001b[1;32m----> 7\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[43mread_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mocr_treshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOCR_TRESHOLD\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m parse_nodes(documents\u001b[38;5;241m=\u001b[39mdocs, llm\u001b[38;5;241m=\u001b[39mllm)\n",
      "File \u001b[1;32md:\\Coding\\git-repo\\github\\ocbc_chatbot\\chatbot-ojk\\utils\\documents_reader.py:24\u001b[0m, in \u001b[0;36mread_documents\u001b[1;34m(path, ocr_treshold)\u001b[0m\n\u001b[0;32m     22\u001b[0m file_metadata \u001b[38;5;241m=\u001b[39m df_metadata[df_metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnew_filename\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m file]\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 24\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mextract_text_from_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mocr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mocr_treshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     document \u001b[38;5;241m=\u001b[39m Document(\n\u001b[0;32m     26\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m     27\u001b[0m         metadata\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m         }\n\u001b[0;32m     37\u001b[0m     )\n\u001b[0;32m     38\u001b[0m     docs_all\u001b[38;5;241m.\u001b[39mappend(document)\n",
      "File \u001b[1;32md:\\Coding\\git-repo\\github\\ocbc_chatbot\\chatbot-ojk\\utils\\documents_reader.py:118\u001b[0m, in \u001b[0;36mextract_text_from_pdf\u001b[1;34m(file_path, ocr, treshold)\u001b[0m\n\u001b[0;32m    116\u001b[0m     page \u001b[38;5;241m=\u001b[39m doc[page_num]\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# Extract text and images from the page\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m     page_text \u001b[38;5;241m=\u001b[39m \u001b[43mextract_text_and_images_from_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mocr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m     text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m page_text \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "File \u001b[1;32md:\\Coding\\git-repo\\github\\ocbc_chatbot\\chatbot-ojk\\utils\\documents_reader.py:96\u001b[0m, in \u001b[0;36mextract_text_and_images_from_page\u001b[1;34m(doc, page, ocr, threshold)\u001b[0m\n\u001b[0;32m     93\u001b[0m image_bytes \u001b[38;5;241m=\u001b[39m image_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image_bytes:\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# Use PaddleOCR to extract text from the image\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m     ocr_result \u001b[38;5;241m=\u001b[39m \u001b[43mocr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mocr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;66;03m# Check if OCR result is valid before processing\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ocr_result \u001b[38;5;129;01mand\u001b[39;00m ocr_result \u001b[38;5;241m!=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m]:\n",
      "File \u001b[1;32mc:\\Users\\acer\\miniconda3\\envs\\chatbot\\lib\\site-packages\\paddleocr\\paddleocr.py:667\u001b[0m, in \u001b[0;36mPaddleOCR.ocr\u001b[1;34m(self, img, det, rec, cls, bin, inv, alpha_color)\u001b[0m\n\u001b[0;32m    665\u001b[0m ocr_res \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, img \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(imgs):\n\u001b[1;32m--> 667\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    668\u001b[0m     dt_boxes, rec_res, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(img, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m    669\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dt_boxes \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rec_res:\n",
      "File \u001b[1;32mc:\\Users\\acer\\miniconda3\\envs\\chatbot\\lib\\site-packages\\paddleocr\\paddleocr.py:657\u001b[0m, in \u001b[0;36mPaddleOCR.ocr.<locals>.preprocess_image\u001b[1;34m(_image)\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_image\u001b[39m(_image):\n\u001b[1;32m--> 657\u001b[0m     _image \u001b[38;5;241m=\u001b[39m \u001b[43malpha_to_color\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_color\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inv:\n\u001b[0;32m    659\u001b[0m         _image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mbitwise_not(_image)\n",
      "File \u001b[1;32mc:\\Users\\acer\\miniconda3\\envs\\chatbot\\lib\\site-packages\\paddleocr\\ppocr\\utils\\utility.py:86\u001b[0m, in \u001b[0;36malpha_to_color\u001b[1;34m(img, alpha_color)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21malpha_to_color\u001b[39m(img, alpha_color\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m)):\n\u001b[1;32m---> 86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m     87\u001b[0m         B, G, R, A \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39msplit(img)\n\u001b[0;32m     88\u001b[0m         alpha \u001b[38;5;241m=\u001b[39m A \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "from utils.documents_reader import read_documents\n",
    "from utils.node_parser import parse_nodes\n",
    "\n",
    "path = './data'\n",
    "\n",
    "if STORE:\n",
    "    docs = read_documents(path=path, ocr_treshold=OCR_TRESHOLD)\n",
    "    nodes = parse_nodes(documents=docs, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Indexing & Storing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the vector index completed.\n"
     ]
    }
   ],
   "source": [
    "from utils.index_store import (store_vector_index, load_vector_index)\n",
    "\n",
    "# store\n",
    "if STORE:\n",
    "    vector_index = store_vector_index(nodes=nodes,embed_model=embedding_llm, delete=DELETE)\n",
    "# load\n",
    "else:\n",
    "    vector_index = load_vector_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Querying**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Retriever**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_retriever = vector_index.as_retriever(similarity_top_k=TOP_K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "qa_prompt = \"\"\"\\\n",
    "Context information is below.\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, \\\n",
    "answer the query asking about banking compliance in Indonesia. \n",
    "Answer the question based on the context information.\n",
    "ALWAYS ANSWER WITH USER'S LANGUAGE.\n",
    "Please provide your answer with [regulation_number](file_url) in metadata \n",
    "(if possible) in the following format:\n",
    "\n",
    "---------------------\n",
    "Answer... \\n\\n\n",
    "Source: [regulation_number](file_url) \\n\n",
    "---------------------\n",
    "\n",
    "Query: {query_str}\n",
    "Answer: \\\n",
    "\"\"\"\n",
    "\n",
    "qa_prompt_tmpl = PromptTemplate(qa_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Query Engine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "vector_query_engine = RetrieverQueryEngine.from_args(retriever=vector_retriever,llm=llm,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: response_synthesizer:text_qa_template<br>**Text:** <br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "{context_str}\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query asking about banking compliance in Indonesia. \n",
      "Answer the question based on the context information.\n",
      "ALWAYS ANSWER WITH USER'S LANGUAGE.\n",
      "Please provide your answer with [regulation_number](file_url) in metadata \n",
      "(if possible) in the following format:\n",
      "\n",
      "---------------------\n",
      "Answer... \n",
      "\n",
      "\n",
      "Source: [regulation_number](file_url) \n",
      "\n",
      "---------------------\n",
      "\n",
      "Query: {query_str}\n",
      "Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: response_synthesizer:refine_template<br>**Text:** <br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original query is as follows: {query_str}\n",
      "We have provided an existing answer: {existing_answer}\n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "{context_msg}\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.prompt_display import display_prompt_dict\n",
    "\n",
    "prompts_dict = vector_query_engine.get_prompts()\n",
    "display_prompt_dict(prompts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"Apa judul peraturan 7/33/PBI/2005?\" # Pencabutan atas Peraturan Bank Indonesia Nomor 5/17/PBI/2003 tentang Persyaratan dan Tata Cara Pelaksanaan Jaminan Pemerintah terhadap Kewajiban Pembayaran Bank Perkreditan Rakyat\n",
    "b = \"Kapan surat edaran No. 15/26/DPbS mulai berlaku?\" # 1 Agustus 2013.\n",
    "c = \"Siapa nama dan jabatannya yang menandatangani surat dengan nomor 1/SEOJK.04/2013?\" # NURHAIDA, kepala eksekutif pengawas pasar modal\n",
    "d = \"Saya ingin menyelenggarakan kegiatan pasar modal berikan saya nomor surat yang membahas mengenai hal ini!\" # Peraturan Pemerintah Nomor 12 Tahun 2004\n",
    "e = \"Berapa persen jaminan moneter pada tanggal 20 Agustus 1958?\" # 7,3%\n",
    "f = \"Surat edaran nomor berapa yang mengatur bank umum syariah dan unit usaha syariah?\" # 15/26/DPbS\n",
    "g = \"Apa kepanjangan dari PAPSI?\" # Pedoman Akuntansi Perbankan Syariah Indonesia\n",
    "h = \"apa judul peraturan nomor 112/KMK.03/2001?\" # Keputusan Menteri Keuangan tentang Pemotongan Pajak Penghasil Pasal 21 atas Penghasilan berupa Uang Pesangon, Uang Tebusan Pensiun, dan Tunjangan Hari Tua atau Jaminan Hari Tua\n",
    "i = \"Saya ingin membuat sistem informasi lembaga jasa keuangan, berikan nomor regulasi dari peraturan yang membahas tentang manejemen risiko nya!\" # 4/POJK.05/2021\n",
    "j = \"Apa kepanjangan dari SWDKLLJ?\" # Sumbangan Wajib Dana Kecelakaan Lalu Lintas Jalan\n",
    "k = \"Berapa nilai SWDKLLJ dari sedan?\" # Rp. 140.000\n",
    "l = \"Apa latar belakang dari peraturan NOMOR 4/POJK.05/2021?\" # dalam bentuk list\n",
    "m = \"Apa itu LJKNB?\" # Lembaga Jasa Keuangan Non Bank\n",
    "n = \"Apakah KMK Nomor 462/KMK.04/1998 masih berlaku\" # tidak\n",
    "o = \"Apa itu Uang Pesangon?\" # penghasilan yang dibayarkan oleh pemberi kerja kepada karyawan dengan nama dan dalam bentuk apapun sehubungan dengan berakhirnya masa kerja atau terjadi pemutusan  hubungan kerja, termasuk uang penghargaan masa kerja dan uang  ganti kerugian\n",
    "p = \"Apa itu CKPN?\" # Cadangan Kerugian Penurunan Nilai.\n",
    "q = \"Kapan, dimana, dan oleh siapa surat nomor PER- 06/BL/2012 ditetapkan?\" # Surat nomor PER-06/BL/2012 ditetapkan pada tanggal 22 November 2012 di Jakarta oleh Ketua Badan Pengawas Pasar Modal dan Lembaga Keuangan.\n",
    "r = \"Apa kepanjangan PSAK?\" # Pernyataan Standar Akuntansi Keuangan\n",
    "s = \"Apa itu 'shahibul maal'?\" # Pemilik dana pihak ketiga\n",
    "\n",
    "query_str = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Nilai SWDKLLJ untuk sedan adalah Rp140.000,00 (seratus empat puluh ribu rupiah).\n",
       "\n",
       "Source: [36/PMK.010/2008](https://www.ojk.go.id/id/regulasi/Documents/Pages/PMK-Nomor-36PMK.010-Tahun-2008-tentang-Besar-Santunan-dan-Sumbangan-Wajib-Dana-Kecelakaan-Lalu-Lintas-Jalan/menas13_1389258036.pdf)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.response.notebook_utils import display_response\n",
    "\n",
    "response_vector = vector_query_engine.query(query_str)\n",
    "display_response(response_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Chat Engine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.storage.chat_store import SimpleChatStore\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "\n",
    "chat_store = SimpleChatStore()\n",
    "memory = ChatMemoryBuffer.from_defaults(\n",
    "    token_limit=10000,\n",
    "    chat_store=chat_store,\n",
    "    chat_store_key=\"user1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.agent.openai import OpenAIAgent\n",
    "# from llama_index.core.agent.legacy.react.base import ReActAgent\n",
    "from llama_index.core.chat_engine import CondenseQuestionChatEngine\n",
    "\n",
    "chat_engine = CondenseQuestionChatEngine.from_defaults(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    query_engine=vector_query_engine,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying with: Apa itu LJKNB?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** LJKNB stands for Lembaga Jasa Keuangan Nonbank, which refers to nonbank financial institutions in Indonesia. These institutions are required to implement risk management in the use of information technology, and they must establish an Information Technology Steering Committee if their total assets exceed Rp1,000,000,000,000,00 (one trillion rupiah). The committee should consist of directors responsible for IT units, directors or officials in charge of risk management, the highest-ranking officials in charge of IT units, and the highest-ranking officials in charge of IT user units."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.response.notebook_utils import display_response\n",
    "\n",
    "response = chat_engine.chat(message=query_str)\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** e840f3a2-cc99-4719-b933-b0b79b5ec7ad<br>**Similarity:** 0.8018614649772644<br>**Text:** MENTERI KEUANGAN\n",
       "REPUBLIK INDONESIA\n",
       "-4-\n",
       " Mobil penumpang angkutan umum sampai dengan 1600 cc\n",
       "g. B...<br>**Metadata:** {'file_name': 'ojk-klasifikasi_bapepam-36_pmk_010_2008-26022008-pmk_nomor_36pmk_010_tahun_2008_tentang_besar_santunan_dan_sumbangan_wajib_dana_kecelakaan_lalu_lintas_jalan_menas13_1389258036_pdf.pdf', 'title': 'Peraturan Menteri Keuangan Nomor 36/PMK.010/2008 tentang Besar Santunan dan Sumbangan Wajib Dana Kecelakaan Lalu Lintas Jalan', 'sector': 'IKNB', 'subsector': 'Asuransi', 'regulation_type': 'Klasifikasi Bapepam', 'regulation_number': '36/PMK.010/2008', 'effective_date': '26 Februari 2008', 'retrieval_score': 0.9084663987159729}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 10989a33-e3d7-4d3d-8f5c-88ad29732ba8<br>**Similarity:** 0.707891047000885<br>**Text:** MENTERI KEUANGAN\n",
       "REPUBLIK INDONESIA\n",
       "memperoleh santunan sebesar Rp 25.000.000,00 (dua puluh lima\n",
       "...<br>**Metadata:** {'file_name': 'ojk-klasifikasi_bapepam-36_pmk_010_2008-26022008-pmk_nomor_36pmk_010_tahun_2008_tentang_besar_santunan_dan_sumbangan_wajib_dana_kecelakaan_lalu_lintas_jalan_menas13_1389258036_pdf.pdf', 'title': 'Peraturan Menteri Keuangan Nomor 36/PMK.010/2008 tentang Besar Santunan dan Sumbangan Wajib Dana Kecelakaan Lalu Lintas Jalan', 'sector': 'IKNB', 'subsector': 'Asuransi', 'regulation_type': 'Klasifikasi Bapepam', 'regulation_number': '36/PMK.010/2008', 'effective_date': '26 Februari 2008', 'retrieval_score': 0.9090815782546997}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 67d00445-e031-4d07-a3ed-a224c28a62a3<br>**Similarity:** 0.6666857600212097<br>**Text:** REPUBLIK INDONESIA\n",
       " Penyelenggaraan Usaha Perasuransian (Lembaran Negara Republik\n",
       "Indonesia Tahun...<br>**Metadata:** {'file_name': 'ojk-klasifikasi_bapepam-36_pmk_010_2008-26022008-pmk_nomor_36pmk_010_tahun_2008_tentang_besar_santunan_dan_sumbangan_wajib_dana_kecelakaan_lalu_lintas_jalan_menas13_1389258036_pdf.pdf', 'title': 'Peraturan Menteri Keuangan Nomor 36/PMK.010/2008 tentang Besar Santunan dan Sumbangan Wajib Dana Kecelakaan Lalu Lintas Jalan', 'sector': 'IKNB', 'subsector': 'Asuransi', 'regulation_type': 'Klasifikasi Bapepam', 'regulation_number': '36/PMK.010/2008', 'effective_date': '26 Februari 2008', 'retrieval_score': 0.8981241583824158}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "\n",
    "for node in response.source_nodes:\n",
    "    display_source_node(node, show_source_metadata=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_store_string = chat_store.json()\n",
    "loaded_chat_store = SimpleChatStore.parse_raw(chat_store_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# check directory exist\n",
    "if not os.path.exists(\"./json_data\"):\n",
    "    os.makedirs(\"./json_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import (\n",
    "    generate_question_context_pairs,\n",
    "    EmbeddingQAFinetuneDataset,\n",
    ")\n",
    "if EVAL:\n",
    "    if STORE:\n",
    "        qa_dataset = generate_question_context_pairs(\n",
    "            nodes=nodes,\n",
    "            llm=llm_eval,\n",
    "            num_questions_per_chunk=2,\n",
    "        )\n",
    "        qa_dataset.save_json(\"./json_data/pg_eval_dataset.json\")\n",
    "    else:\n",
    "        qa_dataset = EmbeddingQAFinetuneDataset.from_json(\"./json_data/pg_eval_dataset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Retrieval Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"hit_rate\", \"mrr\", \"precision\", \"recall\", \"ap\", \"ndcg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluator import get_retrieval_eval_df\n",
    "\n",
    "if EVAL:\n",
    "    vector_retrieval_eval_results = await get_retrieval_eval_df(name=f\"Top-{TOP_K} Eval\", metrics=metrics, retriever=vector_retriever, qa_dataset=qa_dataset)\n",
    "    display(vector_retrieval_eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Response Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import (\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator,\n",
    ")\n",
    "\n",
    "if EVAL:\n",
    "    # model for evaluation\n",
    "    relevancy_evaluator = RelevancyEvaluator(llm=llm_eval)\n",
    "    faithfullness_evaluator = FaithfulnessEvaluator(llm=llm_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Faithfulness Evaluator**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluator import get_response_eval_df\n",
    "\n",
    "if EVAL:\n",
    "    vector_faithfullness_eval_result = faithfullness_evaluator.evaluate_response(response=response_vector, query=query_str)\n",
    "    display(get_response_eval_df(query=query_str, response=response_vector, eval_result=vector_faithfullness_eval_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Source Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluator import get_response_eval_sources_df\n",
    "\n",
    "if EVAL:\n",
    "    relevancy_eval_sources = get_response_eval_sources_df(query=query_str, response=response_vector, evaluator=faithfullness_evaluator)\n",
    "    display(relevancy_eval_sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Relevancy Evaluator**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVAL:\n",
    "    relevancy_eval_result = relevancy_evaluator.evaluate_response(\n",
    "        query=query_str, response=response_vector\n",
    "    )\n",
    "\n",
    "    display(get_response_eval_df(query=query_str, response=response_vector, eval_result=relevancy_eval_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Source Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluator import get_response_eval_sources_df\n",
    "\n",
    "if EVAL:\n",
    "    relevancy_eval_sources = get_response_eval_sources_df(query=query_str, response=response_vector, evaluator=relevancy_evaluator)\n",
    "    display(relevancy_eval_sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Batch Evaluator (Faithfulness, Relevancy, Correctness)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import BatchEvalRunner\n",
    "from utils.evaluator import get_batch_eval_results\n",
    "from utils.evaluator import get_batch_eval_df\n",
    "\n",
    "if EVAL:\n",
    "    runner = BatchEvalRunner(\n",
    "        {\n",
    "            \"faithfulness\": faithfullness_evaluator, \n",
    "            \"relevancy\": relevancy_evaluator,\n",
    "        },\n",
    "        workers=8,\n",
    "    )\n",
    "\n",
    "    vector_eval_results = await get_batch_eval_results(runner=runner, qa_dataset=qa_dataset, query_engine=vector_query_engine, num_queries=NUM_BATCH_EVAL)\n",
    "    display(get_batch_eval_df(vector_eval_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
